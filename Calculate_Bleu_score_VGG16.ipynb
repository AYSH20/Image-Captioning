{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "from caption_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(img_input):\n",
    "    \n",
    "    if img_input.shape != (1, 512):\n",
    "        img_input = img_input.reshape(1, 512)\n",
    "\n",
    "    \n",
    "    assert(img_input.shape == (1, 512))\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    target_seq = np.array([token2idx['<bos>']]).reshape(1, 1)\n",
    "    states_value = encoder_model.predict(img_input)\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = int(np.argmax(output_tokens[0, -1, :]))\n",
    "        sampled_char = idx2token[sampled_token_index]\n",
    "        decoded_sentence += [sampled_char]\n",
    "        if (sampled_char == '<eos>' or len(decoded_sentence) > 30):\n",
    "            stop_condition = True\n",
    "        target_seq = np.array([sampled_token_index]).reshape(1, 1)\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(decoded_sentence[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_captions(model, img_path):   \n",
    "    #img_path = 'data/Arnav_Hankyu_Pulkit2.jpg'\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    features = model.predict(x)\n",
    "    return generate_seq(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fns_list, dev_fns_list, test_fns_list = load_split_lists()\n",
    "train_captions_raw, dev_captions_raw, test_captions_raw = get_caption_split()\n",
    "vocab = create_vocab(train_captions_raw)\n",
    "token2idx, idx2token = vocab_to_index(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/engine/topology.py:1541: UserWarning: The list of outputs passed to the model is redundant. All outputs should only appear once. Found: [<tf.Tensor 'img_embedding/Relu:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'img_embedding/Relu:0' shape=(?, 300) dtype=float32>]\n",
      "  ' Found: ' + str(self.outputs))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "encoder_model = load_model('saved_models/encoder_model.h5')\n",
    "decoder_model = load_model('saved_models/decoder_model.h5')\n",
    "VGG16_model = VGG16(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n"
     ]
    }
   ],
   "source": [
    "bleu1 = np.zeros(len(test_fns_list))\n",
    "bleu2 = np.zeros(len(test_fns_list))\n",
    "bleu3 = np.zeros(len(test_fns_list))\n",
    "bleu4 = np.zeros(len(test_fns_list))\n",
    "\n",
    "for i, filename in enumerate(test_fns_list):\n",
    "    if i%100 == 0:\n",
    "        print(i, \"images processed\")\n",
    "    candidates = [get_captions(VGG16_model, \"data/Flicker8k_Dataset/\"+filename).split()]*5\n",
    "    references = []    \n",
    "    for j, caption in enumerate(test_captions_raw[filename]):\n",
    "        references.append(caption[:-1].split())\n",
    "    bleu1[i] = corpus_bleu(references, candidates, weights=(1, 0, 0, 0))\n",
    "    bleu2[i] = corpus_bleu(references, candidates, weights=(0, 1, 0, 0))\n",
    "    bleu3[i] = corpus_bleu(references, candidates, weights=(0, 0, 1, 0))\n",
    "    bleu4[i] = corpus_bleu(references, candidates, weights=(0, 0, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16\n",
      "Bleu1 Score:  0.10055779131835635\n",
      "Bleu2 Score:  0.30610375207562346\n",
      "Bleu3 Score:  0.44651866284028346\n",
      "Bleu4 Score:  0.5399454288695043\n"
     ]
    }
   ],
   "source": [
    "print(\"VGG16\")\n",
    "print(\"Bleu1 Score: \", bleu1.mean())\n",
    "print(\"Bleu2 Score: \", bleu2.mean())\n",
    "print(\"Bleu3 Score: \", bleu3.mean())\n",
    "print(\"Bleu4 Score: \", bleu4.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run it until here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/engine/topology.py:1541: UserWarning: The list of outputs passed to the model is redundant. All outputs should only appear once. Found: [<tf.Tensor 'img_embedding_1/Relu:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'img_embedding_1/Relu:0' shape=(?, 300) dtype=float32>]\n",
      "  ' Found: ' + str(self.outputs))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "encoder_model = load_model('saved_models/encoder_model_VGG19.h5')\n",
    "decoder_model = load_model('saved_models/decoder_model_VGG19.h5')\n",
    "VGG19_model = VGG19(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n"
     ]
    }
   ],
   "source": [
    "for i, filename in enumerate(test_fns_list):\n",
    "    if i%100 == 0:\n",
    "        print(i, \"images processed\")\n",
    "    candidate = get_captions(VGG19_model, \"data/Flicker8k_Dataset/\"+filename).split()\n",
    "    for j, caption in enumerate(test_captions_raw[filename]):\n",
    "        reference = caption[:-1].split()\n",
    "        bleu1[i*5+j] = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "        bleu2[i*5+j] = sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))\n",
    "        bleu3[i*5+j] = sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))\n",
    "        bleu4[i*5+j] = sentence_bleu(reference, candidate, weights=(0, 0, 0, 1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG19\n",
      "Bleu1 Score:  0.10948410832083721\n",
      "Bleu2 Score:  0.9566502893814297\n",
      "Bleu3 Score:  0.9566502893814297\n",
      "Bleu4 Score:  0.9566502893814297\n"
     ]
    }
   ],
   "source": [
    "print(\"VGG19\")\n",
    "print(\"Bleu1 Score: \", bleu1.mean())\n",
    "print(\"Bleu2 Score: \", bleu2.mean())\n",
    "print(\"Bleu3 Score: \", bleu3.mean())\n",
    "print(\"Bleu4 Score: \", bleu4.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(img_input):\n",
    "    \n",
    "    if img_input.shape != (1, 2048):\n",
    "        img_input = img_input.reshape(1, 2048)\n",
    "\n",
    "    \n",
    "    assert(img_input.shape == (1, 2048))\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    target_seq = np.array([token2idx['<bos>']]).reshape(1, 1)\n",
    "    states_value = encoder_model.predict(img_input)\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = int(np.argmax(output_tokens[0, -1, :]))\n",
    "        sampled_char = idx2token[sampled_token_index]\n",
    "        decoded_sentence += [sampled_char]\n",
    "        if (sampled_char == '<eos>' or len(decoded_sentence) > 30):\n",
    "            stop_condition = True\n",
    "        target_seq = np.array([sampled_token_index]).reshape(1, 1)\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(decoded_sentence[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/engine/topology.py:1541: UserWarning: The list of outputs passed to the model is redundant. All outputs should only appear once. Found: [<tf.Tensor 'img_embedding_2/Relu:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'img_embedding_2/Relu:0' shape=(?, 300) dtype=float32>]\n",
      "  ' Found: ' + str(self.outputs))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "encoder_model = load_model('saved_models/encoder_model_ResNet50.h5')\n",
    "decoder_model = load_model('saved_models/decoder_model_ResNet50.h5')\n",
    "ResNet50_model = ResNet50(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n"
     ]
    }
   ],
   "source": [
    "for i, filename in enumerate(test_fns_list):\n",
    "    if i%100 == 0:\n",
    "        print(i, \"images processed\")\n",
    "    candidate = get_captions(ResNet50_model, \"data/Flicker8k_Dataset/\"+filename).split()\n",
    "    for j, caption in enumerate(test_captions_raw[filename]):\n",
    "        reference = caption[:-1].split()\n",
    "        bleu1[i*5+j] = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "        bleu2[i*5+j] = sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))\n",
    "        bleu3[i*5+j] = sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))\n",
    "        bleu4[i*5+j] = sentence_bleu(reference, candidate, weights=(0, 0, 0, 1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet50\n",
      "Bleu1 Score:  0.10003500434853214\n",
      "Bleu2 Score:  0.9325552583614292\n",
      "Bleu3 Score:  0.9325552583614292\n",
      "Bleu4 Score:  0.9325552583614292\n"
     ]
    }
   ],
   "source": [
    "print(\"ResNet50\")\n",
    "print(\"Bleu1 Score: \", bleu1.mean())\n",
    "print(\"Bleu2 Score: \", bleu2.mean())\n",
    "print(\"Bleu3 Score: \", bleu3.mean())\n",
    "print(\"Bleu4 Score: \", bleu4.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/engine/topology.py:1541: UserWarning: The list of outputs passed to the model is redundant. All outputs should only appear once. Found: [<tf.Tensor 'img_embedding_3/Relu:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'img_embedding_3/Relu:0' shape=(?, 300) dtype=float32>]\n",
      "  ' Found: ' + str(self.outputs))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.xception import Xception\n",
    "from keras.applications.xception import preprocess_input\n",
    "\n",
    "encoder_model = load_model('saved_models/encoder_model_ResNet50.h5')\n",
    "decoder_model = load_model('saved_models/decoder_model_ResNet50.h5')\n",
    "Xception_model = Xception(weights='imagenet', include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n"
     ]
    }
   ],
   "source": [
    "for i, filename in enumerate(test_fns_list):\n",
    "    if i%100 == 0:\n",
    "        print(i, \"images processed\")\n",
    "    candidate = get_captions(Xception_model, \"data/Flicker8k_Dataset/\"+filename).split()\n",
    "    for j, caption in enumerate(test_captions_raw[filename]):\n",
    "        reference = caption[:-1].split()\n",
    "        bleu1[i*5+j] = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "        bleu2[i*5+j] = sentence_bleu(reference, candidate, weights=(0, 1, 0, 0))\n",
    "        bleu3[i*5+j] = sentence_bleu(reference, candidate, weights=(0, 0, 1, 0))\n",
    "        bleu4[i*5+j] = sentence_bleu(reference, candidate, weights=(0, 0, 0, 1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xception\n",
      "Bleu1 Score:  0.15455040708797751\n",
      "Bleu2 Score:  0.9731898361621599\n",
      "Bleu3 Score:  0.9731898361621599\n",
      "Bleu4 Score:  0.9731898361621599\n"
     ]
    }
   ],
   "source": [
    "print(\"Xception\")\n",
    "print(\"Bleu1 Score: \", bleu1.mean())\n",
    "print(\"Bleu2 Score: \", bleu2.mean())\n",
    "print(\"Bleu3 Score: \", bleu3.mean())\n",
    "print(\"Bleu4 Score: \", bleu4.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
