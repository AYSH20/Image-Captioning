{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(img_input):\n",
    "    \n",
    "    if img_input.shape != (1, 512):\n",
    "        img_input = img_input.reshape(1, 512)\n",
    "\n",
    "    \n",
    "    assert(img_input.shape == (1, 512))\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    target_seq = np.array([token2idx['<bos>']]).reshape(1, 1)\n",
    "    states_value = encoder_model.predict(img_input)\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = int(np.argmax(output_tokens[0, -1, :]))\n",
    "        sampled_char = idx2token[sampled_token_index]\n",
    "        decoded_sentence += [sampled_char]\n",
    "        if (sampled_char == '<eos>' or len(decoded_sentence) > 30):\n",
    "            stop_condition = True\n",
    "        target_seq = np.array([sampled_token_index]).reshape(1, 1)\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(decoded_sentence[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "def get_captions(img_path):   \n",
    "    #img_path = 'data/Arnav_Hankyu_Pulkit2.jpg'\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "\n",
    "    features = VGG16_model.predict(x)\n",
    "    return generate_seq(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caption_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hankyu\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\topology.py:1541: UserWarning: The list of outputs passed to the model is redundant. All outputs should only appear once. Found: [<tf.Tensor 'img_embedding_1/Relu:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'img_embedding_1/Relu:0' shape=(?, 300) dtype=float32>]\n",
      "  ' Found: ' + str(self.outputs))\n",
      "C:\\Users\\hankyu\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "train_fns_list, dev_fns_list, test_fns_list = load_split_lists()\n",
    "train_captions_raw, dev_captions_raw, test_captions_raw = get_caption_split()\n",
    "vocab = create_vocab(train_captions_raw)\n",
    "token2idx, idx2token = vocab_to_index(vocab)\n",
    "\n",
    "encoder_model = load_model('encoder_model.h5')\n",
    "decoder_model = load_model('decoder_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use `corpus_bleu`, number of references per image (5) should match the number of candidates (5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 images processed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hankyu\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\nltk\\translate\\bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 images processed\n",
      "200 images processed\n",
      "300 images processed\n",
      "400 images processed\n",
      "500 images processed\n",
      "600 images processed\n",
      "700 images processed\n",
      "800 images processed\n",
      "900 images processed\n"
     ]
    }
   ],
   "source": [
    "bleu_scores = np.zeros(len(test_fns_list))\n",
    "for i, filename in enumerate(test_fns_list):\n",
    "    if i%100 == 0:\n",
    "        print(i, \"images processed\")\n",
    "    references = []\n",
    "    for caption in test_captions_raw[filename]:\n",
    "        references.append(caption[:-1].split())\n",
    "    candidates = [get_captions(\"data/Flicker8k_Dataset/\"+filename).split()] * 5\n",
    "    bleu_scores[i] = corpus_bleu(references, candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu Score:  0.5399454288695043\n"
     ]
    }
   ],
   "source": [
    "print(\"Bleu Score: \", bleu_scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
