{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.load('train_dev_test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder_output = all_data['train_encoder_output']\n",
    "train_decoder_input = all_data['train_decoder_input']\n",
    "train_decoder_target = all_data['train_decoder_target'][:,1:,:]\n",
    "validation_encoder_output = all_data['validation_encoder_output']\n",
    "validation_decoder_input = all_data['validation_decoder_input']\n",
    "validation_decoder_target = all_data['validation_decoder_target'][:,1:,:]\n",
    "test_encoder_output = all_data['test_encoder_output']\n",
    "test_decoder_input = all_data['test_decoder_input']\n",
    "test_decoder_target = all_data['test_decoder_target'][:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Encoder Output (30000, 512)\n",
      "Train Decoder Input (30000, 38)\n",
      "Train Decoder Target (30000, 38, 2531)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Encoder Output\", train_encoder_output.shape)\n",
    "print(\"Train Decoder Input\", train_decoder_input.shape)\n",
    "print(\"Train Decoder Target\", train_decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkitmaloo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/pulkitmaloo/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from caption_utils import *\n",
    "train_fns_list, dev_fns_list, test_fns_list = load_split_lists()\n",
    "\n",
    "train_captions_raw, dev_captions_raw, test_captions_raw = get_caption_split()\n",
    "vocab = create_vocab(train_captions_raw)\n",
    "token2idx, idx2token = vocab_to_index(vocab)    \n",
    "captions_data = (train_captions_raw.copy(), dev_captions_raw.copy(), test_captions_raw.copy())\n",
    "train_captions, dev_captions, test_captions = process_captions(captions_data, token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, RepeatVector, Concatenate, Merge, Masking\n",
    "from keras.layers import LSTM, GRU, Embedding, TimeDistributed, Bidirectional\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 300\n",
    "lstm_size = 300\n",
    "vocab_size = len(vocab)\n",
    "max_length = train_decoder_target.shape[1]\n",
    "lstm_layers = 2\n",
    "\n",
    "lr = 0.001\n",
    "dropout_rate = 0.2\n",
    "batch_size = 32\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input shape (?, 512)\n",
      "(?, 300)\n"
     ]
    }
   ],
   "source": [
    "# Image -> Image embedding\n",
    "image_input = Input(shape=(train_encoder_output.shape[1], ), name='image_input')\n",
    "print(\"Image Input shape\", image_input.shape)\n",
    "img_emb = Dense(emb_size, activation='relu', name='img_embedding')(image_input)\n",
    "print(img_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption Input Shape (?, ?)\n",
      "(?, ?, 300)\n"
     ]
    }
   ],
   "source": [
    "# Sentence to Word embedding\n",
    "caption_inputs = Input(shape=(None, ), name='caption_input')\n",
    "print(\"Caption Input Shape\", caption_inputs.shape)\n",
    "emb_layer = Embedding(input_dim=vocab_size, output_dim=emb_size, name='Embedding')\n",
    "word_emb = emb_layer(caption_inputs)\n",
    "print(word_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_cell = Bidirectional(LSTM(lstm_size, return_sequences=True, return_state=True, name='decoder', \n",
    "                                  dropout=dropout_rate, recurrent_dropout=dropout_rate), \n",
    "                             merge_mode='ave')\n",
    "encoder_states = [img_emb, img_emb]\n",
    "decoder_out = decoder_cell(word_emb, initial_state = encoder_states + encoder_states)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 2531)\n"
     ]
    }
   ],
   "source": [
    "output_layer = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "decoder_out = output_layer(decoder_out)\n",
    "print(decoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[image_input,caption_inputs], outputs=[decoder_out])\n",
    "\n",
    "model.compile(optimizer=rmsprop,\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "caption_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_input (InputLayer)        (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Embedding)           (None, None, 300)    759300      caption_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "img_embedding (Dense)           (None, 300)          153900      image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 300),  1442400     Embedding[0][0]                  \n",
      "                                                                 img_embedding[0][0]              \n",
      "                                                                 img_embedding[0][0]              \n",
      "                                                                 img_embedding[0][0]              \n",
      "                                                                 img_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 2531)   761831      bidirectional_1[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 3,117,431\n",
      "Trainable params: 3,117,431\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "#plot_model(model, to_file='model2.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.bi_final.hdf5', \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Train on 30000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      " - 149s - loss: 0.7298 - acc: 0.1883 - val_loss: 0.3640 - val_acc: 0.2567\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.36403, saving model to saved_models/weights.best.VGG16.bi_final.hdf5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/engine/topology.py:2368: UserWarning: Layer bidirectional_1 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'img_embedding/Relu:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'img_embedding/Relu:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'img_embedding/Relu:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'img_embedding/Relu:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30\n",
      " - 145s - loss: 0.2876 - acc: 0.2678 - val_loss: 0.2161 - val_acc: 0.2824\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.36403 to 0.21609, saving model to saved_models/weights.best.VGG16.bi_final.hdf5\n",
      "Epoch 3/30\n",
      " - 144s - loss: 0.1923 - acc: 0.2844 - val_loss: 0.1562 - val_acc: 0.2928\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.21609 to 0.15617, saving model to saved_models/weights.best.VGG16.bi_final.hdf5\n",
      "Epoch 4/30\n"
     ]
    }
   ],
   "source": [
    "model.fit([train_encoder_output, train_decoder_input], [train_decoder_target], \n",
    "           validation_data=([validation_encoder_output, validation_decoder_input], [validation_decoder_target]),\n",
    "           epochs=n_epochs, batch_size=batch_size, callbacks=[checkpointer], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(image_input, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(lstm_size, ))\n",
    "decoder_state_input_c = Input(shape=(lstm_size, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_cell(word_emb, initial_state=decoder_states_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = output_layer(decoder_outputs)\n",
    "decoder_model = Model([caption_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.VGG16.bi_final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(img_input):\n",
    "    if img_input.shape != (1, 512):\n",
    "        img_input = img_input.reshape(1, 512)\n",
    "    \n",
    "    assert(img_input.shape == (1, 512))\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    target_seq = np.array([token2idx['<bos>']]).reshape(1, 1)\n",
    "    states_value = encoder_model.predict(img_input)\n",
    "    \n",
    "    neg_log_proba = 0.\n",
    "    while not stop_condition:\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        output_tokens = np.squeeze(output_tokens)\n",
    "        \n",
    "        sampled_token_index = int(np.argmax(output_tokens))\n",
    "        neg_log_proba -= np.log(output_tokens[sampled_token_index])\n",
    "        \n",
    "        sampled_char = idx2token[sampled_token_index]\n",
    "\n",
    "        decoded_sentence += [sampled_char]\n",
    "\n",
    "        if (sampled_char == '<eos>' or len(decoded_sentence) > 30):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.array([sampled_token_index]).reshape(1, 1)\n",
    "\n",
    "        states_value = [h, c]\n",
    "#     print(neg_log_proba/len(decoded_sentence)**0.7)\n",
    "    return neg_log_proba/len(decoded_sentence)**0.7, ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    print(generate_seq(test_encoder_output[i*5, :]))\n",
    "    print('*'*30)\n",
    "    for j in range(5):\n",
    "        print(intseq_to_caption(idx2token, test_captions[test_fns_list[i]][j]))\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    for j in range(5):\n",
    "        print(intseq_to_caption(idx2token, train_captions[train_fns_list[i]][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0bfd1cdf0eba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bi_test.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mmodel\u001b[0m  \u001b[0;31m# deletes the existing model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bi_test.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model.save(\"bi_test.h5\")\n",
    "\n",
    "del model  # deletes the existing model\n",
    "model = load_model('bi_test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/Arnav_Hankyu_Pulkit2.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-36b79c6a84c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'data/Arnav_Hankyu_Pulkit2.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/preprocessing/image.py\u001b[0m in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, target_size, interpolation)\u001b[0m\n\u001b[1;32m    347\u001b[0m         raise ImportError('Could not import PIL.Image. '\n\u001b[1;32m    348\u001b[0m                           'The use of `array_to_img` requires PIL.')\n\u001b[0;32m--> 349\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_image\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mgrayscale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'L'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2542\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2543\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2544\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/Arnav_Hankyu_Pulkit2.jpg'"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "VGG16_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "img_path = 'data/Arnav_Hankyu_Pulkit2.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "features = VGG16_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_seq(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkitmaloo/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1539: UserWarning: The list of outputs passed to the model is redundant. All outputs should only appear once. Found: [<tf.Tensor 'img_embedding_11/Relu:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'img_embedding_11/Relu:0' shape=(?, 300) dtype=float32>]\n",
      "  ' Found: ' + str(self.outputs))\n",
      "/Users/pulkitmaloo/anaconda3/lib/python3.6/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "encoder_model = load_model('saved_models/encoder_model.h5')\n",
    "decoder_model = load_model('saved_models/decoder_model.h5')\n",
    "\n",
    "img_input = train_encoder_output[1, :]\n",
    "if img_input.shape != (1, 512):\n",
    "    img_input = img_input.reshape(1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_size = 5\n",
    "max_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_sentence(sent):\n",
    "    return ' '.join([idx2token[idx] for idx in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_one_step(sent, states_value, beam_size=5, len_norm=True, alpha=1):\n",
    "    \"\"\" \n",
    "    sent: [neg_log_prob, [1, ...]]\n",
    "    states_value: [h, c]\n",
    "    return list of sent\n",
    "    \"\"\"\n",
    "    last_word_idx = sent[1][-1]\n",
    "    \n",
    "    # If the last word was already <eos>\n",
    "    if last_word_idx == token2idx['<eos>']:\n",
    "        return [sent], [None, None]\n",
    "    \n",
    "    target_seq = np.array([last_word_idx]).reshape(1, 1)\n",
    "    output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "    output_tokens = np.squeeze(output_tokens)\n",
    "    \n",
    "    predicted_sentences = []    \n",
    "    output_tokens_beam = np.argpartition(-output_tokens, beam_size)[: beam_size]\n",
    "    \n",
    "    for predict_idx in output_tokens_beam:\n",
    "        if predict_idx in [0, 1, 3]:\n",
    "            continue\n",
    "            \n",
    "        neg_log_prob = sent[0] * max(len(sent[1])-1, 1)**alpha - np.log(output_tokens[predict_idx])\n",
    "        new_sent = sent[1] + [int(predict_idx)]\n",
    "        \n",
    "        if len_norm:\n",
    "            neg_log_prob = neg_log_prob / max(len(new_sent)-1, 1)**alpha\n",
    "            \n",
    "        predicted_sentences.append([neg_log_prob, new_sent])\n",
    "        \n",
    "#     print(\"\\nfrom\", idx_to_sentence(sent[1]), \"\\npredicting\", \n",
    "#           list(map(lambda x: idx_to_sentence(x[1]), predicted_sentences)))\n",
    "    \n",
    "    return (predicted_sentences, [h, c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(img_input, beam_size=5, max_length=20, len_norm=True, alpha=0.7):\n",
    "    if img_input.shape != (1, 512):\n",
    "        img_input = img_input.reshape(1, 512)    \n",
    "    assert(img_input.shape == (1, 512))\n",
    "    states_value_initial = encoder_model.predict(img_input)\n",
    "    \n",
    "    beg_sent = [0., [token2idx['<bos>']]]\n",
    "#     print(beg_sent)\n",
    "    top_sentences, states_value = decoder_one_step(beg_sent, states_value_initial, beam_size, len_norm, alpha)\n",
    "#     print(list(map(lambda x: idx_to_sentence(x[1]), top_sentences)))\n",
    "    \n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        new_top_sentences = []\n",
    "        \n",
    "        for sent in top_sentences:\n",
    "            \n",
    "            if sent[1][-1] == token2idx['<eos>']:\n",
    "                new_top_sentences += [sent]\n",
    "                continue\n",
    "                \n",
    "            predicted_sent, new_states_value = decoder_one_step(sent, states_value, beam_size, len_norm, alpha)\n",
    "            new_top_sentences += predicted_sent\n",
    "        \n",
    "        top_sentences = sorted(new_top_sentences, key=lambda x: x[0])[: beam_size]\n",
    "        assert len(top_sentences) == beam_size\n",
    "        states_value = new_states_value\n",
    "\n",
    "#         print(idx_to_sentence(top_sentences[0][1]))\n",
    "        \n",
    "        # Update stop condition\n",
    "        eos_cnt = 0\n",
    "        any_max_len = False\n",
    "        for sent in top_sentences:\n",
    "            if sent[1][-1] == token2idx['<eos>']:\n",
    "                eos_cnt += 1\n",
    "            if len(sent[1]) >= max_length:\n",
    "                any_max_len = True\n",
    "                print('Max len reached')\n",
    "                break\n",
    "        \n",
    "        if any_max_len or (eos_cnt == beam_size):\n",
    "            stop_condition = True        \n",
    "            \n",
    "    return top_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d1733fc18546>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_to_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_decoder_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Greedy\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_encoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Beam Search\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtop_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_encoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(1, 100, 5):\n",
    "    print(\"Original\\n\", seq_to_sentence(np.argmax(train_decoder_target[i, :], -1)))\n",
    "    print(\"Greedy\\n\", generate_seq(train_encoder_output[i, :]), alpha=0.7)\n",
    "    print(\"Beam Search\")\n",
    "    top_sentences = beam_search(train_encoder_output[i, :], beam_size=3, max_length=20, alpha=0.7)\n",
    "    for sent in top_sentences:\n",
    "        print(sent[0], seq_to_sentence(sent[1][1: ]))\n",
    "    print('*'*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam Search\n",
      "1.2379845331780728 a brown dog is running in the snow <eos>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.896992697035087e-07"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-15.054422453045845)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -2, -3])"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
