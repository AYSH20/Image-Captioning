{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- encoder_input_data: 2D array of shape `(num_images * 5, 512)`\n",
    "- decoder_input_data: 3D array of shape `(num_captions, max_words_in_sentence, num_words)`\n",
    "- decoder_output_data: same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Preprocessing\n",
    "\n",
    "obtain bottleneck features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since there are 5 captions per image, duplicate the bottleneck features\n",
    "def duplicate_bottleneck_features(features):\n",
    "    num_captions = 5 # 5 stands for number of captions per image\n",
    "    num_rows = features.shape[0] * num_captions \n",
    "\n",
    "    features_dup = np.zeros((num_rows, features.shape[1]))\n",
    "    for i, image in enumerate(features):\n",
    "        for j in range(num_captions):\n",
    "            features_dup[i*num_captions + j] = image\n",
    "    return features_dup    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/Flicker8k_bottleneck_features_VGG16_avgpooling.npz')\n",
    "bottleneck_features_train = bottleneck_features[\"train\"]\n",
    "bottleneck_features_validation = bottleneck_features[\"validation\"]\n",
    "bottleneck_features_test = bottleneck_features[\"test\"]\n",
    "\n",
    "bottleneck_features_train_dup = duplicate_bottleneck_features(bottleneck_features_train)\n",
    "bottleneck_features_validation_dup = duplicate_bottleneck_features(bottleneck_features_validation)\n",
    "bottleneck_features_test_dup = duplicate_bottleneck_features(bottleneck_features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 512)\n",
      "(5000, 512)\n",
      "(5000, 512)\n"
     ]
    }
   ],
   "source": [
    "print(bottleneck_features_train_dup.shape)\n",
    "print(bottleneck_features_validation_dup.shape)\n",
    "print(bottleneck_features_test_dup.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkitmaloo/Box Sync/Workspace/Github/Projects/CV-Project/caption_utils.py:91: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
      "  assert(idx2token[idx] == token, \"token2idx and idx2token not equivalent\")\n",
      "/Users/pulkitmaloo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/pulkitmaloo/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from caption_utils import *\n",
    "train_fns_list, dev_fns_list, test_fns_list = load_split_lists()\n",
    "\n",
    "train_captions_raw, dev_captions_raw, test_captions_raw = get_caption_split()\n",
    "vocab = create_vocab(train_captions_raw)\n",
    "token2idx, idx2token = vocab_to_index(vocab)     \n",
    "captions_data = (train_captions_raw.copy(), dev_captions_raw.copy(), test_captions_raw.copy())\n",
    "train_captions, dev_captions, test_captions = process_captions(captions_data, token2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2531\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caption Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions_onehot = np.load('preprocessed_captions/Flicker8k_onehot_'+str(len(vocab))+'_words.npz')\n",
    "train_captions_onehot = captions_onehot[\"train\"]\n",
    "validation_captions_onehot = captions_onehot[\"validation\"]\n",
    "test_captions_onehot = captions_onehot[\"test\"]\n",
    "\n",
    "train_captions_onehot = train_captions_onehot.astype(np.float32)\n",
    "validation_captions_onehot = validation_captions_onehot.astype(np.float32)\n",
    "test_captions_onehot = test_captions_onehot.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 39, 2531)\n",
      "(5000, 39, 2531)\n",
      "(5000, 39, 2531)\n"
     ]
    }
   ],
   "source": [
    "print(train_captions_onehot.shape)\n",
    "print(validation_captions_onehot.shape)\n",
    "print(test_captions_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32 float32 float32\n"
     ]
    }
   ],
   "source": [
    "print(train_captions_onehot.dtype, validation_captions_onehot.dtype, test_captions_onehot.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def captions_onehot_split(captions_onehot):\n",
    "    \"\"\" returns decoder input data and decoder target data \"\"\"\n",
    "    return captions_onehot[:, :-1, :], captions_onehot[:, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training captions - > decoder input, target data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_decoder_input, train_decoder_target = captions_onehot_split(train_captions_onehot)\n",
    "validation_decoder_input, validation_decoder_target = captions_onehot_split(validation_captions_onehot)\n",
    "test_decoder_input, test_decoder_target = captions_onehot_split(test_captions_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder_output = bottleneck_features_train_dup.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 38)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_decoder_input = np.argmax(test_decoder_input, axis=-1)\n",
    "train_decoder_input = np.argmax(train_decoder_input, axis=-1)\n",
    "validation_decoder_input = np.argmax(validation_decoder_input, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder Input (30000, 38, 2531) float32\n",
      "Decoder Target (30000, 39, 2531) float32\n",
      "Encoder Output (30000, 512) float32\n"
     ]
    }
   ],
   "source": [
    "print(\"Decoder Input\", train_decoder_input.shape, train_decoder_input.dtype)\n",
    "print(\"Decoder Target\", train_decoder_target.shape, train_decoder_target.dtype)\n",
    "print(\"Encoder Output\", train_encoder_output.shape, train_encoder_output.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 5  # Number of epochs to train for.\n",
    "latent_dim = 300  # Latent dimensionality of the encoding space.\n",
    "num_samples = train_encoder_output.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------\n",
    "# Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, BatchNormalization, RepeatVector, Concatenate, TimeDistributed, Merge\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "emb_size = 150\n",
    "lstm_size = 300\n",
    "vocab_size = len(vocab)\n",
    "max_length = train_decoder_target.shape[1]\n",
    "print(max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_1:0\", shape=(?, 512), dtype=float32)\n",
      "Tensor(\"repeat_vector_1/Tile:0\", shape=(?, 1, 150), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "image_input = Input(shape=(train_encoder_output.shape[1], ), dtype='float32')\n",
    "print(image_input)\n",
    "img_emb = Dense(emb_size, activation='relu')(image_input)\n",
    "img_emb = RepeatVector(1)(img_emb)\n",
    "print(img_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input_2:0\", shape=(?, 38), dtype=float32)\n",
      "Tensor(\"embedding_1/Gather:0\", shape=(?, 38, 150), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "caption_inputs = Input(shape=(max_length-1, ), dtype='float32')\n",
    "print(caption_inputs)\n",
    "word_emb = Embedding(input_dim=vocab_size, output_dim=emb_size)(caption_inputs)\n",
    "print(word_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concatenate_1/concat:0\", shape=(?, 39, 150), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "seq_input = Concatenate(axis=1)([img_emb, word_emb])\n",
    "print(seq_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 39, 2531)\n"
     ]
    }
   ],
   "source": [
    "gru_cell = GRU(lstm_size, return_sequences=True)(seq_input)\n",
    "seq_out = TimeDistributed(Dense(vocab_size))(gru_cell)\n",
    "print(seq_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-bf05efa055d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m Model(inputs=[train_encoder_output, train_decoder_input],\n\u001b[0;32m----> 2\u001b[0;31m               outputs=[train_decoder_target])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m   1526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m         \u001b[0;31m# Check for redundancy in inputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1529\u001b[0m             raise ValueError('The list of inputs passed to the model '\n\u001b[1;32m   1530\u001b[0m                              \u001b[0;34m'is redundant. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "Model(inputs=[train_encoder_output, train_decoder_input],\n",
    "              outputs=[train_decoder_target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_captions = np.zeros((train_captions_onehot.shape[0], train_captions_onehot.shape[1]))\n",
    "for i, onehot_caption in enumerate(train_captions_onehot):\n",
    "    train_captions[i] = np.argmax(onehot_caption, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkitmaloo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "\n",
    "# Image -> Image embedding\n",
    "img_emb = Sequential()\n",
    "img_emb.add(Dense(emb_size, input_dim=train_encoder_output.shape[1], activation='relu'))\n",
    "img_emb.add(RepeatVector(1))\n",
    "\n",
    "# Sentence to Word embedding\n",
    "word_emb = Sequential()\n",
    "word_emb.add(Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=max_length-1))\n",
    "\n",
    "# Merge img_emb and word_emb\n",
    "seq_in = Sequential()\n",
    "seq_in.add(Merge([img_emb, word_emb], mode='concat', concat_axis=1))\n",
    "#seq_in.add(Concatenate([img_emb, word_emb]))\n",
    "\n",
    "# RNN Layer\n",
    "seq_in.add(GRU(lstm_size, return_sequences=True))\n",
    "seq_in.add(TimeDistributed(Dense(vocab_size)))\n",
    "\n",
    "seq_in.compile(optimizer='adam',\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_1_input to have 2 dimensions, but got array with shape (30000, 38, 2531)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-a5c75839c6f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseq_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_encoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_decoder_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_decoder_target\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1594\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1424\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1427\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1428\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    108\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_1_input to have 2 dimensions, but got array with shape (30000, 38, 2531)"
     ]
    }
   ],
   "source": [
    "seq_in.fit([train_encoder_output, train_decoder_input], [train_decoder_target], epochs=5, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 150)               76950     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 1, 150)            0         \n",
      "=================================================================\n",
      "Total params: 76,950\n",
      "Trainable params: 76,950\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 38, 150)           379650    \n",
      "=================================================================\n",
      "Total params: 379,650\n",
      "Trainable params: 379,650\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_1 (Merge)              (None, 39, 150)           0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 39, 300)           405900    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 39, 2531)          761831    \n",
      "=================================================================\n",
      "Total params: 1,624,331\n",
      "Trainable params: 1,624,331\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(img_emb.summary())\n",
    "print(word_emb.summary())\n",
    "print(seq_in.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(seq_in, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keras.legacy.layers.Merge"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A `Merge` layer can be used to merge a list of tensors\n",
      "    into a single tensor, following some merge `mode`.\n",
      "    # Example\n",
      "    ```python\n",
      "    model1 = Sequential()\n",
      "    model1.add(Dense(32, input_dim=32))\n",
      "    model2 = Sequential()\n",
      "    model2.add(Dense(32, input_dim=32))\n",
      "    merged_model = Sequential()\n",
      "    merged_model.add(Merge([model1, model2], mode='concat', concat_axis=1))\n",
      "    ```\n",
      "    # Arguments\n",
      "        layers: Can be a list of Keras tensors or\n",
      "            a list of layer instances. Must be more\n",
      "            than one layer/tensor.\n",
      "        mode: String or lambda/function. If string, must be one\n",
      "            of: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot', 'max'.\n",
      "            If lambda/function, it should take as input a list of tensors\n",
      "            and return a single tensor.\n",
      "        concat_axis: Integer, axis to use in mode `concat`.\n",
      "        dot_axes: Integer or tuple of integers,\n",
      "            axes to use in mode `dot` or `cos`.\n",
      "        output_shape: Either a shape tuple (tuple of integers),\n",
      "            or a lambda/function\n",
      "            to compute `output_shape`\n",
      "            (only if merge mode is a lambda/function).\n",
      "            If the argument is a tuple,\n",
      "            it should be expected output shape, *not* including the batch size\n",
      "            (same convention as the `input_shape` argument in layers).\n",
      "            If the argument is callable,\n",
      "            it should take as input a list of shape tuples\n",
      "            (1:1 mapping to input tensors)\n",
      "            and return a single shape tuple, including the\n",
      "            batch size (same convention as the\n",
      "            `compute_output_shape` method of layers).\n",
      "        node_indices: Optional list of integers containing\n",
      "            the output node index for each input layer\n",
      "            (in case some input layers have multiple output nodes).\n",
      "            will default to an array of 0s if not provided.\n",
      "        tensor_indices: Optional list of indices of output tensors\n",
      "            to consider for merging\n",
      "            (in case some input layer node returns multiple tensors).\n",
      "        output_mask: Mask or lambda/function to compute the output mask (only\n",
      "            if merge mode is a lambda/function). If the latter case, it should\n",
      "            take as input a list of masks and return a single mask.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(Merge.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
