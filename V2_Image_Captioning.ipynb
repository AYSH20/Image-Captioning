{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.load('train_dev_test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder_output = all_data['train_encoder_output']\n",
    "train_decoder_input = all_data['train_decoder_input']\n",
    "train_decoder_target = all_data['train_decoder_target'][:,1:,:]\n",
    "validation_encoder_output = all_data['validation_encoder_output']\n",
    "validation_decoder_input = all_data['validation_decoder_input']\n",
    "validation_decoder_target = all_data['validation_decoder_target'][:,1:,:]\n",
    "test_encoder_output = all_data['test_encoder_output']\n",
    "test_decoder_input = all_data['test_decoder_input']\n",
    "test_decoder_target = all_data['test_decoder_target'][:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Encoder Output (30000, 512)\n",
      "Train Decoder Input (30000, 38)\n",
      "Train Decoder Target (30000, 38, 2531)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Encoder Output\", train_encoder_output.shape)\n",
    "print(\"Train Decoder Input\", train_decoder_input.shape)\n",
    "print(\"Train Decoder Target\", train_decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caption_utils import *\n",
    "train_fns_list, dev_fns_list, test_fns_list = load_split_lists()\n",
    "\n",
    "train_captions_raw, dev_captions_raw, test_captions_raw = get_caption_split()\n",
    "vocab = create_vocab(train_captions_raw)\n",
    "token2idx, idx2token = vocab_to_index(vocab)    \n",
    "captions_data = (train_captions_raw.copy(), dev_captions_raw.copy(), test_captions_raw.copy())\n",
    "train_captions, dev_captions, test_captions = process_captions(captions_data, token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, RepeatVector, Concatenate, Merge, Masking\n",
    "from keras.layers import LSTM, GRU, Embedding, TimeDistributed, Bidirectional\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 300\n",
    "lstm_size = 300\n",
    "vocab_size = len(vocab)\n",
    "max_length = train_decoder_target.shape[1]\n",
    "lstm_layers = 2\n",
    "\n",
    "lr = 0.001\n",
    "dropout_rate = 0.2\n",
    "batch_size = 32\n",
    "n_epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input shape (?, 512)\n",
      "(?, 300)\n"
     ]
    }
   ],
   "source": [
    "# Image -> Image embedding\n",
    "image_input = Input(shape=(train_encoder_output.shape[1], ), name='image_input')\n",
    "print(\"Image Input shape\", image_input.shape)\n",
    "img_emb = Dense(emb_size, activation='relu', name='img_embedding')(image_input)\n",
    "print(img_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption Input Shape (?, ?)\n",
      "(?, ?, 300)\n"
     ]
    }
   ],
   "source": [
    "# Sentence to Word embedding\n",
    "caption_inputs = Input(shape=(None, ), name='caption_input')\n",
    "print(\"Caption Input Shape\", caption_inputs.shape)\n",
    "emb_layer = Embedding(input_dim=vocab_size, output_dim=emb_size, name='Embedding')\n",
    "word_emb = emb_layer(caption_inputs)\n",
    "print(word_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_cell = LSTM(lstm_size, return_sequences=True, return_state=True, name='decoder', dropout=dropout_rate, recurrent_dropout=dropout_rate)\n",
    "encoder_states = [img_emb, img_emb]\n",
    "decoder_out, _, _ = decoder_cell(word_emb, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 2531)\n"
     ]
    }
   ],
   "source": [
    "output_layer = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "decoder_out = output_layer(decoder_out)\n",
    "print(decoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[image_input,caption_inputs], outputs=[decoder_out])\n",
    "\n",
    "model.compile(optimizer=rmsprop,\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "caption_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_input (InputLayer)        (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Embedding)           (None, None, 300)    759300      caption_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "img_embedding (Dense)           (None, 300)          153900      image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder (LSTM)                  [(None, None, 300),  721200      Embedding[0][0]                  \n",
      "                                                                 img_embedding[0][0]              \n",
      "                                                                 img_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 2531)   761831      decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,396,231\n",
      "Trainable params: 2,396,231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "#plot_model(model, to_file='model2.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.final.hdf5', \n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30000 samples, validate on 5000 samples\n",
      "Epoch 1/30\n",
      " - 82s - loss: 1.1760 - acc: 0.0974 - val_loss: 1.0501 - val_acc: 0.1105\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.05011, saving model to saved_models/weights.best.VGG16.final.hdf5\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/engine/topology.py:2368: UserWarning: Layer decoder was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'img_embedding/Relu:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'img_embedding/Relu:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 81s - loss: 0.9920 - acc: 0.1163 - val_loss: 0.9902 - val_acc: 0.1178\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.05011 to 0.99022, saving model to saved_models/weights.best.VGG16.final.hdf5\n",
      "Epoch 3/30\n",
      " - 81s - loss: 0.9372 - acc: 0.1231 - val_loss: 0.9664 - val_acc: 0.1206\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.99022 to 0.96639, saving model to saved_models/weights.best.VGG16.final.hdf5\n",
      "Epoch 4/30\n",
      " - 81s - loss: 0.9054 - acc: 0.1273 - val_loss: 0.9547 - val_acc: 0.1222\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.96639 to 0.95472, saving model to saved_models/weights.best.VGG16.final.hdf5\n",
      "Epoch 5/30\n",
      " - 81s - loss: 0.8835 - acc: 0.1307 - val_loss: 0.9492 - val_acc: 0.1220\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.95472 to 0.94920, saving model to saved_models/weights.best.VGG16.final.hdf5\n",
      "Epoch 6/30\n",
      " - 81s - loss: 0.8652 - acc: 0.1332 - val_loss: 0.9426 - val_acc: 0.1235\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.94920 to 0.94260, saving model to saved_models/weights.best.VGG16.final.hdf5\n",
      "Epoch 7/30\n",
      " - 81s - loss: 0.8507 - acc: 0.1355 - val_loss: 0.9428 - val_acc: 0.1239\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/30\n",
      " - 81s - loss: 0.8371 - acc: 0.1378 - val_loss: 0.9419 - val_acc: 0.1238\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.94260 to 0.94186, saving model to saved_models/weights.best.VGG16.final.hdf5\n",
      "Epoch 9/30\n",
      " - 81s - loss: 0.8267 - acc: 0.1394 - val_loss: 0.9412 - val_acc: 0.1238\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.94186 to 0.94121, saving model to saved_models/weights.best.VGG16.final.hdf5\n",
      "Epoch 10/30\n",
      " - 81s - loss: 0.8174 - acc: 0.1409 - val_loss: 0.9453 - val_acc: 0.1240\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/30\n",
      " - 81s - loss: 0.8077 - acc: 0.1429 - val_loss: 0.9453 - val_acc: 0.1231\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/30\n",
      " - 81s - loss: 0.8000 - acc: 0.1439 - val_loss: 0.9454 - val_acc: 0.1239\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/30\n",
      " - 81s - loss: 0.7929 - acc: 0.1452 - val_loss: 0.9474 - val_acc: 0.1236\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/30\n",
      " - 81s - loss: 0.7861 - acc: 0.1464 - val_loss: 0.9502 - val_acc: 0.1233\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/30\n",
      " - 81s - loss: 0.7795 - acc: 0.1474 - val_loss: 0.9492 - val_acc: 0.1235\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/30\n",
      " - 81s - loss: 0.7735 - acc: 0.1482 - val_loss: 0.9523 - val_acc: 0.1234\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/30\n",
      " - 81s - loss: 0.7698 - acc: 0.1491 - val_loss: 0.9587 - val_acc: 0.1234\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/30\n",
      " - 81s - loss: 0.7655 - acc: 0.1501 - val_loss: 0.9594 - val_acc: 0.1228\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/30\n",
      " - 81s - loss: 0.7608 - acc: 0.1510 - val_loss: 0.9610 - val_acc: 0.1225\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/30\n",
      " - 81s - loss: 0.7560 - acc: 0.1516 - val_loss: 0.9634 - val_acc: 0.1225\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/30\n",
      " - 81s - loss: 0.7518 - acc: 0.1524 - val_loss: 0.9625 - val_acc: 0.1230\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-ee60fb1b616f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit([train_encoder_output, train_decoder_input], [train_decoder_target], \n\u001b[1;32m      2\u001b[0m            \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalidation_encoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_decoder_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvalidation_decoder_target\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m            epochs=n_epochs, batch_size=batch_size, callbacks=[checkpointer], verbose=2)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit([train_encoder_output, train_decoder_input], [train_decoder_target], \n",
    "           validation_data=([validation_encoder_output, validation_decoder_input], [validation_decoder_target]),\n",
    "           epochs=n_epochs, batch_size=batch_size, callbacks=[checkpointer], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/engine/topology.py:1541: UserWarning: The list of outputs passed to the model is redundant. All outputs should only appear once. Found: [<tf.Tensor 'img_embedding/Relu:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'img_embedding/Relu:0' shape=(?, 300) dtype=float32>]\n",
      "  ' Found: ' + str(self.outputs))\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(image_input, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "img_embedding (Dense)        (None, 300)               153900    \n",
      "=================================================================\n",
      "Total params: 153,900\n",
      "Trainable params: 153,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(lstm_size, ))\n",
    "decoder_state_input_c = Input(shape=(lstm_size, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_cell(word_emb, initial_state=decoder_states_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = output_layer(decoder_outputs)\n",
    "decoder_model = Model([caption_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "caption_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Embedding)           (None, None, 300)    759300      caption_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder (LSTM)                  [(None, None, 300),  721200      Embedding[0][0]                  \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 2531)   761831      decoder[1][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,242,331\n",
      "Trainable params: 2,242,331\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.VGG16.final.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(img_input):\n",
    "    if img_input.shape != (1, 512):\n",
    "        img_input = img_input.reshape(1, 512)\n",
    "    \n",
    "    assert(img_input.shape == (1, 512))\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    target_seq = np.array([token2idx['<bos>']]).reshape(1, 1)\n",
    "    states_value = encoder_model.predict(img_input)\n",
    "\n",
    "    while not stop_condition:\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = int(np.argmax(output_tokens[0, -1, :]))\n",
    "\n",
    "        sampled_char = idx2token[sampled_token_index]\n",
    "\n",
    "        decoded_sentence += [sampled_char]\n",
    "\n",
    "        if (sampled_char == '<eos>' or len(decoded_sentence) > 30):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.array([sampled_token_index]).reshape(1, 1)\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two dogs are running in the snow <eos>\n",
      "******************************\n",
      "<bos> the dogs are in the snow in front of a fence <eos>\n",
      "<bos> the dogs play on the snow <eos>\n",
      "<bos> two brown dogs playfully fight in the snow <eos>\n",
      "<bos> two brown dogs wrestle in the snow <eos>\n",
      "<bos> two dogs playing in the snow <eos>\n",
      "\n",
      "a dog with a blue ball in its mouth is laying on the ground <eos>\n",
      "******************************\n",
      "<bos> a brown and white dog swimming towards some in the pool <eos>\n",
      "<bos> a dog in a swimming pool swims toward <unk> <unk> <unk> see <eos>\n",
      "<bos> a dog swims in a pool near a person <eos>\n",
      "<bos> small dog is paddling through the water in a pool <eos>\n",
      "<bos> the small brown and white dog is in the pool <eos>\n",
      "\n",
      "a man in a green shirt and a woman in a white shirt and sunglasses is sitting on a bench <eos>\n",
      "******************************\n",
      "<bos> a man and a woman in festive costumes dancing <eos>\n",
      "<bos> a man and a woman with <unk> on her head dance <eos>\n",
      "<bos> a man and a woman wearing <unk> costumes and dancing in a crowd of onlookers <eos>\n",
      "<bos> one performer wearing a <unk> headdress dancing with another performer in the streets <eos>\n",
      "<bos> two people are dancing with drums on the right and a crowd behind them <eos>\n",
      "\n",
      "a woman and a woman are standing on a bench <eos>\n",
      "******************************\n",
      "<bos> a couple of people sit outdoors at a table with an umbrella and talk <eos>\n",
      "<bos> three people are sitting at an outside picnic bench with an umbrella <eos>\n",
      "<bos> three people sit at an outdoor cafe <eos>\n",
      "<bos> three people sit at an outdoor table in front of a building painted like the <unk> jack <eos>\n",
      "<bos> three people sit at a picnic table outside of a building painted like a <unk> jack <eos>\n",
      "\n",
      "a football player in red is running with a football in the background <eos>\n",
      "******************************\n",
      "<bos> a man is wearing a sooners red football shirt and helmet <eos>\n",
      "<bos> a oklahoma sooners football player wearing his jersey number <unk> <eos>\n",
      "<bos> a sooners football player <unk> the number <unk> and black <unk> <eos>\n",
      "<bos> guy in red and white football uniform <eos>\n",
      "<bos> the american footballer is wearing a red and white strip <eos>\n",
      "\n",
      "a brown dog is running through the grass <eos>\n",
      "******************************\n",
      "<bos> a brown dog running <eos>\n",
      "<bos> a brown dog running over grass <eos>\n",
      "<bos> a brown dog with its front paws off the ground on a grassy surface near red and purple flowers <eos>\n",
      "<bos> a dog runs across a grassy lawn near some flowers <eos>\n",
      "<bos> a yellow dog is playing in a grassy area near flowers <eos>\n",
      "\n",
      "a woman and a woman are posing for a picture <eos>\n",
      "******************************\n",
      "<bos> a girl with dark brown hair and eyes in a blue scarf is standing next to a girl in a fur <unk> coat <eos>\n",
      "<bos> an asian boy and an asian girl are smiling in a crowd of people <eos>\n",
      "<bos> the girls <unk> in the crowd <eos>\n",
      "<bos> two dark haired girls are in a crowd <eos>\n",
      "<bos> two girls are looking past each other in different directions while standing in a crowd <eos>\n",
      "\n",
      "a brown dog with a blue collar is standing on its back <eos>\n",
      "******************************\n",
      "<bos> a dog with its mouth <unk> <eos>\n",
      "<bos> brown and white dog yawning <eos>\n",
      "<bos> close up of dog in <unk> with mouth open <eos>\n",
      "<bos> dog <unk> <eos>\n",
      "<bos> the dog <unk> mouth is open like he is yawning <eos>\n",
      "\n",
      "a black dog is running through the grass <eos>\n",
      "******************************\n",
      "<bos> a black dog emerges from the water onto the sand holding a white object in its mouth <eos>\n",
      "<bos> a black dog emerges from the water with a white ball in its mouth <eos>\n",
      "<bos> a black dog on a beach carrying a ball in its mouth <eos>\n",
      "<bos> a black dog walking out of the water with a white ball in his mouth <eos>\n",
      "<bos> the black dog jumps out of the water with something in its mouth <eos>\n",
      "\n",
      "a man in a white shirt and a white shirt is playing basketball <eos>\n",
      "******************************\n",
      "<bos> a player from the white and green <unk> team dribbles down court <unk> by a player from the other team <eos>\n",
      "<bos> four basketball players in action <eos>\n",
      "<bos> four men playing basketball two from each team <eos>\n",
      "<bos> two boys in green and white uniforms play basketball with two boys in blue and white uniforms <eos>\n",
      "<bos> young men playing basketball in a competition <eos>\n",
      "\n",
      "a group of people are standing on a sidewalk <eos>\n",
      "******************************\n",
      "<bos> a group of <unk> stand around as a lady puts her hand near the mouth of a statue <eos>\n",
      "<bos> a woman is making a statue <unk> to kiss her hand beside four boys at a bench <eos>\n",
      "<bos> a woman posing with a statue alongside a group of boys <eos>\n",
      "<bos> a woman with a backpack leans <unk> a statue while a group of boys sit on a bench talking <eos>\n",
      "<bos> woman gets her hand <unk> by living statue street artist <eos>\n",
      "\n",
      "a man with a blue shirt and a man in a blue shirt <eos>\n",
      "******************************\n",
      "<bos> a man in yellow <unk> <eos>\n",
      "<bos> a man wearing a yellow shirt with a <unk> look on his face <eos>\n",
      "<bos> the man in the yellow t shirt is pulling a large <unk> on his face <eos>\n",
      "<bos> there is a man wearing a yellow shirt and <unk> <eos>\n",
      "<bos> this man is smiling very big at the camera <eos>\n",
      "\n",
      "a white dog is jumping on a bed <eos>\n",
      "******************************\n",
      "<bos> a dog lays on a <unk> on the porch <eos>\n",
      "<bos> a dog rolls on a <unk> placed on a porch and <unk> his back <eos>\n",
      "<bos> a spotted dog rolling over on a pad placed on a porch <eos>\n",
      "<bos> a white dog rolling on its back on a porch <eos>\n",
      "<bos> the spotted dog rolls on its back <eos>\n",
      "\n",
      "a woman in a black shirt is sitting on a bench with a woman in a black jacket <eos>\n",
      "******************************\n",
      "<bos> a man is standing on a sidewalk in the background with a blurry image of another man in the foreground <eos>\n",
      "<bos> a man stands at a busy bus stop <eos>\n",
      "<bos> a man stands next to a bus <eos>\n",
      "<bos> people in the city <eos>\n",
      "<bos> people walking down the street and past an open bus <eos>\n",
      "\n",
      "a man in a green hat and hat is standing on a rocky beach <eos>\n",
      "******************************\n",
      "<bos> a girl wearing a brown cap red sneakers and a dark green coat sits on a rock bench <eos>\n",
      "<bos> a hiker in a jacket and knit cap sits with a backpack on atop a stone with a cloudy sky in the background <eos>\n",
      "<bos> a woman in a knit cap and green coat its on a stone block looking out <eos>\n",
      "<bos> a woman with her backpack sits on a large rock and looks down over the mountains <eos>\n",
      "<bos> the lady is sitting on a rock <eos>\n",
      "\n",
      "a man and woman are standing in front of a crowd of people <eos>\n",
      "******************************\n",
      "<bos> a man helps another man tie a red ribbon onto his arm <eos>\n",
      "<bos> a man helps tie a red ribbon around another man <unk> right arm during a street parade <eos>\n",
      "<bos> a man is <unk> a red arm band around another mans arm in the street <eos>\n",
      "<bos> one man helps another <unk> a red ribbon to his <unk> in the midst of a large group of people <eos>\n",
      "<bos> two men stand together one is putting something red on his arm <eos>\n",
      "\n",
      "a brown dog is jumping in the air to catch a ball in its mouth <eos>\n",
      "******************************\n",
      "<bos> a dog with a frisbee in front of a brown dog <eos>\n",
      "<bos> a large black dog is catching a frisbee while a large brown dog follows <unk> after <eos>\n",
      "<bos> two dark colored dogs romp in the grass with a blue frisbee <eos>\n",
      "<bos> two dogs are catching blue <unk> in grass <eos>\n",
      "<bos> two dogs are playing one is catching a frisbee <eos>\n",
      "\n",
      "a young boy in a blue shirt is holding a <unk> <eos>\n",
      "******************************\n",
      "<bos> a boy holds a picture over his head <eos>\n",
      "<bos> a boy in a long <unk> shirt is holding a box above his head <eos>\n",
      "<bos> a boy is inside holding something on top of his head <eos>\n",
      "<bos> a little boy holds a decorated cardboard box on his head <eos>\n",
      "<bos> a young boy with a <unk> cardboard box atop his head <eos>\n",
      "\n",
      "two brown dogs run through the grass <eos>\n",
      "******************************\n",
      "<bos> a big dog chases a little dog on the grass <eos>\n",
      "<bos> a black dog is chasing a smaller brown dog over grass <eos>\n",
      "<bos> the large brown dog is chasing after the little brown dog <eos>\n",
      "<bos> two brown dogs run through the grass together <eos>\n",
      "<bos> two dogs play in the grass <eos>\n",
      "\n",
      "a man in a black shirt is sitting on a bed <eos>\n",
      "******************************\n",
      "<bos> a man is sitting on the floor outside a door and his head on his chin <eos>\n",
      "<bos> a man sits against a yellow wall wearing all black <eos>\n",
      "<bos> a man wearing a dark blue hat sits on the ground and leans against a building <eos>\n",
      "<bos> man with black hat coat and pants sitting next to the door of a building <eos>\n",
      "<bos> the man in the black hat is sitting on the floor beside the green door <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(generate_seq(test_encoder_output[i*5, :]))\n",
    "    print('*'*30)\n",
    "    for j in range(5):\n",
    "        print(intseq_to_caption(idx2token, test_captions[test_fns_list[i]][j]))\n",
    "    print()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos> a black dog is running after a white dog in the snow <eos>\n",
      "<bos> black dog chasing brown dog through snow <eos>\n",
      "<bos> two dogs chase each other across the snowy ground <eos>\n",
      "<bos> two dogs play together in the snow <eos>\n",
      "<bos> two dogs running through a low lying body of water <eos>\n",
      "<bos> a little baby plays croquet <eos>\n",
      "<bos> a little girl plays croquet next to a truck <eos>\n",
      "<bos> the child is playing <unk> by the truck <eos>\n",
      "<bos> the kid is in front of a car with a put and a ball <eos>\n",
      "<bos> the little boy is playing with a croquet <unk> and ball beside the car <eos>\n",
      "<bos> a brown dog in the snow has something hot pink in its mouth <eos>\n",
      "<bos> a brown dog in the snow holding a pink hat <eos>\n",
      "<bos> a brown dog is holding a pink shirt in the snow <eos>\n",
      "<bos> a dog is carrying something pink in its mouth while walking through the snow <eos>\n",
      "<bos> a dog with something pink in its mouth is looking forward <eos>\n",
      "<bos> a brown dog is running along a beach <eos>\n",
      "<bos> a brown dog wearing a black collar running across the beach <eos>\n",
      "<bos> a dog walks on the sand near the water <eos>\n",
      "<bos> brown dog running on the beach <eos>\n",
      "<bos> the large brown dog is running on the beach by the ocean <eos>\n",
      "<bos> a black and white dog with a red frisbee standing on a sandy beach <eos>\n",
      "<bos> a dog <unk> a red disc on a beach <eos>\n",
      "<bos> a dog with a red frisbee flying in the air <eos>\n",
      "<bos> dog catching a red frisbee <eos>\n",
      "<bos> the black dog is dropping a red disc on a beach <eos>\n",
      "<bos> a cyclist wearing a red helmet is riding on the pavement <eos>\n",
      "<bos> a girl is riding a bike on the street while wearing a red helmet <eos>\n",
      "<bos> a person on a bike wearing a red helmet riding down a street <eos>\n",
      "<bos> a woman wears a red helmet and blue shirt as she goes for a bike ride in the shade <eos>\n",
      "<bos> person in blue shirt and red helmet riding bike down the road <eos>\n",
      "<bos> a man dressed in a purple shirt and red bandanna smiles at the people watching him <eos>\n",
      "<bos> a man on the street wearing leather <unk> and a <unk> <unk> <eos>\n",
      "<bos> a man wearing a purple shirt and black leather <unk> poses for the camera <eos>\n",
      "<bos> man dressed in leather <unk> and purple shirt stands in front of onlookers <eos>\n",
      "<bos> there is a man in a purple shirt leather <unk> and a red bandanna standing near other men <eos>\n",
      "<bos> a boy wearing a red t shirt is running through woodland <eos>\n",
      "<bos> a child runs near some trees <eos>\n",
      "<bos> a young boy is dancing around <eos>\n",
      "<bos> a young boy with a red short sleeved shirt and jeans runs by some trees <eos>\n",
      "<bos> the little boy in the red shirt stops to smile for the camera <eos>\n",
      "<bos> a girl in a white dress <eos>\n",
      "<bos> a little girl in white is looking back at the camera while carrying a water <unk> <eos>\n",
      "<bos> a smiling young girl in <unk> is playing ball <eos>\n",
      "<bos> a young girl wearing white looks at the camera as she plays <eos>\n",
      "<bos> the girl is holding a green ball <eos>\n",
      "<bos> a skier in a yellow jacket is airborne above the mountains <eos>\n",
      "<bos> a skier jumps high in the air with a view of the mountains <eos>\n",
      "<bos> a skiing man in a <unk> jacket jumps very high and it looks as though he is flying <eos>\n",
      "<bos> <unk> is high in the air doing a ski jump <eos>\n",
      "<bos> the skier in the green jacket and white pants appears to almost fly into the sky <eos>\n",
      "<bos> a photographer looks over the hills <eos>\n",
      "<bos> a woman in a red jacket is <unk> a natural landscape <eos>\n",
      "<bos> a woman with a camera looks out over rolling hills <eos>\n",
      "<bos> a woman with a camera on a tripod is looking at the view <eos>\n",
      "<bos> lady in red shirt has her camera set up in the field to record something <eos>\n",
      "<bos> a bunch of girls in cheerleader outfits <eos>\n",
      "<bos> a large group of cheerleaders walking in a parade <eos>\n",
      "<bos> cheerleaders perform <eos>\n",
      "<bos> many cheerleaders wearing black walk down the street <eos>\n",
      "<bos> parade of cheerleaders wearing black pink and white uniforms <eos>\n",
      "<bos> a blue boat with a yellow canopy is floating on calm waters <eos>\n",
      "<bos> a boat in the water <eos>\n",
      "<bos> a boat with a roof on green water <eos>\n",
      "<bos> the boat is in the middle of the water <eos>\n",
      "<bos> the <unk> boat floats on the lake <eos>\n",
      "<bos> a dog catches a frisbee in midair <eos>\n",
      "<bos> a dog catching a frisbee <eos>\n",
      "<bos> a terrier <unk> catches a frisbee in the air <eos>\n",
      "<bos> a white and black dog catching a frisbee <eos>\n",
      "<bos> a white dog is leaping in the air with a green object in its mouth <eos>\n",
      "<bos> a little old lady sitting next to an advertisement <eos>\n",
      "<bos> an asian woman waiting at an <unk> train stop <eos>\n",
      "<bos> an old woman sits in a <unk> station next to a backlit advertisement <eos>\n",
      "<bos> a woman sits in a subway station <eos>\n",
      "<bos> a woman with an umbrella is sitting at a station with an <unk> <unk> on the wall <eos>\n",
      "<bos> a blond girl wearing a green jacket walks on a trail along side a metal fence <eos>\n",
      "<bos> a girl in a green coat walks down a rural road playing a flute <eos>\n",
      "<bos> a young girl in a parka playing a flute while walking by a fenced in field <eos>\n",
      "<bos> girl in green and blue jacket walking past an enclosed field <eos>\n",
      "<bos> girl playing flute as she walks by fence in rural area <eos>\n",
      "<bos> a family of <unk> people including four children pose in front of a brick fireplace with a white <unk> <eos>\n",
      "<bos> a family poses in front of the fireplace and christmas tree <eos>\n",
      "<bos> a family posing by the <unk> and christmas tree <eos>\n",
      "<bos> a happy family poses by the fireplace <eos>\n",
      "<bos> two couples and four kids pose for a family picture <eos>\n",
      "<bos> man in a sweater pointing at the camera <eos>\n",
      "<bos> one man is posing with arms outstretched and finger pointed while another stares from behind him <eos>\n",
      "<bos> the man in the black hat stands behind the man who is pointing his finger <eos>\n",
      "<bos> two men look toward the camera while the one in front points his <unk> finger <eos>\n",
      "<bos> two men one wearing a black hat while the one in front points standing in a hallway <eos>\n",
      "<bos> a dog looks <unk> at the brown dog <unk> his area <eos>\n",
      "<bos> a large brown dog is looking at a medium sized black dog <eos>\n",
      "<bos> a small black dog looks at a larger brown dog in a grassy field <eos>\n",
      "<bos> the big brown dog looks at the small black dog in tall grass <eos>\n",
      "<bos> there is a big dog looking at a little dog <eos>\n",
      "<bos> three children in a field with white flowers <eos>\n",
      "<bos> three children one with a stuffed <unk> in a field of flowers <eos>\n",
      "<bos> three children play in the garden <eos>\n",
      "<bos> three children pose among <unk> <eos>\n",
      "<bos> three kids <unk> with a toy cat in a garden <eos>\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    for j in range(5):\n",
    "        print(intseq_to_caption(idx2token, train_captions[train_fns_list[i]][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow-3.5/lib/python3.5/site-packages/keras/models.py:255: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model.save(\"test.h5\")\n",
    "\n",
    "del model  # deletes the existing model\n",
    "model = load_model('test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "VGG16_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "\n",
    "img_path = 'data/Arnav_Hankyu_Pulkit2.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "features = VGG16_model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "two men and a woman are posing for a picture <eos>\n"
     ]
    }
   ],
   "source": [
    "print(generate_seq(features))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
