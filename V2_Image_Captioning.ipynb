{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = np.load('train_dev_test.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder_output = all_data['train_encoder_output']\n",
    "train_decoder_input = all_data['train_decoder_input']\n",
    "train_decoder_target = all_data['train_decoder_target'][:,1:,:]\n",
    "validation_encoder_output = all_data['validation_encoder_output']\n",
    "validation_decoder_input = all_data['validation_decoder_input']\n",
    "validation_decoder_target = all_data['validation_decoder_target'][:,1:,:]\n",
    "test_encoder_output = all_data['test_encoder_output']\n",
    "test_decoder_input = all_data['test_decoder_input']\n",
    "test_decoder_target = all_data['test_decoder_target'][:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Encoder Output (30000, 512)\n",
      "Train Decoder Input (30000, 38)\n",
      "Train Decoder Target (30000, 38, 2531)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Encoder Output\", train_encoder_output.shape)\n",
    "print(\"Train Decoder Input\", train_decoder_input.shape)\n",
    "print(\"Train Decoder Target\", train_decoder_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkitmaloo/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/Users/pulkitmaloo/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from caption_utils import *\n",
    "train_fns_list, dev_fns_list, test_fns_list = load_split_lists()\n",
    "\n",
    "train_captions_raw, dev_captions_raw, test_captions_raw = get_caption_split()\n",
    "vocab = create_vocab(train_captions_raw)\n",
    "token2idx, idx2token = vocab_to_index(vocab)    \n",
    "captions_data = (train_captions_raw.copy(), dev_captions_raw.copy(), test_captions_raw.copy())\n",
    "train_captions, dev_captions, test_captions = process_captions(captions_data, token2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, RepeatVector, Concatenate, Merge, Masking\n",
    "from keras.layers import LSTM, GRU, Embedding, TimeDistributed, Bidirectional\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 300\n",
    "lstm_size = 300\n",
    "vocab_size = len(vocab)\n",
    "max_length = train_decoder_target.shape[1]\n",
    "lstm_layers = 2\n",
    "\n",
    "lr = 0.001\n",
    "dropout_rate = 0.2\n",
    "batch_size = 64\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input shape (?, 512)\n",
      "(?, 300)\n"
     ]
    }
   ],
   "source": [
    "# Image -> Image embedding\n",
    "image_input = Input(shape=(train_encoder_output.shape[1], ), name='image_input')\n",
    "print(\"Image Input shape\", image_input.shape)\n",
    "img_emb = Dense(emb_size, activation='relu', name='img_embedding')(image_input)\n",
    "print(img_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption Input Shape (?, ?)\n",
      "(?, ?, 300)\n"
     ]
    }
   ],
   "source": [
    "# Sentence to Word embedding\n",
    "caption_inputs = Input(shape=(None, ), name='caption_input')\n",
    "print(\"Caption Input Shape\", caption_inputs.shape)\n",
    "emb_layer = Embedding(input_dim=vocab_size, output_dim=emb_size, name='Embedding')\n",
    "word_emb = emb_layer(caption_inputs)\n",
    "print(word_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_cell = LSTM(lstm_size, return_sequences=True, return_state=True, name='decoder')\n",
    "encoder_states = [img_emb, img_emb]\n",
    "decoder_out, _, _ = decoder_cell(word_emb, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 2531)\n"
     ]
    }
   ],
   "source": [
    "output_layer = TimeDistributed(Dense(vocab_size, activation='softmax'))\n",
    "decoder_out = output_layer(decoder_out)\n",
    "print(decoder_out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmsprop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[image_input,caption_inputs], outputs=[decoder_out])\n",
    "\n",
    "model.compile(optimizer=rmsprop,\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "caption_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "image_input (InputLayer)        (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Embedding)           (None, None, 300)    759300      caption_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "img_embedding (Dense)           (None, 300)          153900      image_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder (LSTM)                  [(None, None, 300),  721200      Embedding[0][0]                  \n",
      "                                                                 img_embedding[0][0]              \n",
      "                                                                 img_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 2531)   761831      decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,396,231\n",
      "Trainable params: 2,396,231\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "plot_model(model, to_file='model2.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "10/10 [==============================] - 1s 142ms/step - loss: 2.1776 - acc: 0.0000e+00\n",
      "Epoch 2/20\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 2.0515 - acc: 0.0500\n",
      "Epoch 3/20\n",
      "10/10 [==============================] - 0s 39ms/step - loss: 1.8687 - acc: 0.0500\n",
      "Epoch 4/20\n",
      "10/10 [==============================] - 1s 59ms/step - loss: 1.6067 - acc: 0.0500\n",
      "Epoch 5/20\n",
      "10/10 [==============================] - 0s 37ms/step - loss: 1.2925 - acc: 0.0421\n",
      "Epoch 6/20\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 1.0524 - acc: 0.0447\n",
      "Epoch 7/20\n",
      "10/10 [==============================] - 0s 40ms/step - loss: 0.9584 - acc: 0.0316\n",
      "Epoch 8/20\n",
      "10/10 [==============================] - 0s 38ms/step - loss: 0.9204 - acc: 0.0474\n",
      "Epoch 9/20\n",
      "10/10 [==============================] - 0s 38ms/step - loss: 0.8935 - acc: 0.0447\n",
      "Epoch 10/20\n",
      "10/10 [==============================] - 0s 37ms/step - loss: 0.8712 - acc: 0.0632\n",
      "Epoch 11/20\n",
      "10/10 [==============================] - 0s 38ms/step - loss: 0.8557 - acc: 0.0526\n",
      "Epoch 12/20\n",
      "10/10 [==============================] - 0s 44ms/step - loss: 0.8398 - acc: 0.0763\n",
      "Epoch 13/20\n",
      "10/10 [==============================] - 0s 41ms/step - loss: 0.8306 - acc: 0.0579\n",
      "Epoch 14/20\n",
      "10/10 [==============================] - 1s 54ms/step - loss: 0.8174 - acc: 0.0763\n",
      "Epoch 15/20\n",
      "10/10 [==============================] - 0s 42ms/step - loss: 0.8100 - acc: 0.0684\n",
      "Epoch 16/20\n",
      "10/10 [==============================] - 0s 47ms/step - loss: 0.7933 - acc: 0.0737\n",
      "Epoch 17/20\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.7861 - acc: 0.0711\n",
      "Epoch 18/20\n",
      "10/10 [==============================] - 1s 50ms/step - loss: 0.7740 - acc: 0.0737\n",
      "Epoch 19/20\n",
      "10/10 [==============================] - 1s 57ms/step - loss: 0.7665 - acc: 0.0737\n",
      "Epoch 20/20\n",
      "10/10 [==============================] - 0s 35ms/step - loss: 0.7626 - acc: 0.0737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x4cd4f39b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([train_encoder_output[:10,:], train_decoder_input[:10,:]], [train_decoder_target[:10,:,:]], \n",
    "#           validation_data=([validation_encoder_output, validation_decoder_input], [validation_decoder_target]),\n",
    "           epochs=n_epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkitmaloo/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1539: UserWarning: The list of outputs passed to the model is redundant. All outputs should only appear once. Found: [<tf.Tensor 'img_embedding/Relu:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'img_embedding/Relu:0' shape=(?, 300) dtype=float32>]\n",
      "  ' Found: ' + str(self.outputs))\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(image_input, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "image_input (InputLayer)     (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "img_embedding (Dense)        (None, 300)               153900    \n",
      "=================================================================\n",
      "Total params: 153,900\n",
      "Trainable params: 153,900\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_state_input_h = Input(shape=(lstm_size, ))\n",
    "decoder_state_input_c = Input(shape=(lstm_size, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_cell(word_emb, initial_state=decoder_states_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = output_layer(decoder_outputs)\n",
    "decoder_model = Model([caption_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "caption_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding (Embedding)           (None, None, 300)    759300      caption_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 300)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder (LSTM)                  [(None, None, 300),  721200      Embedding[0][0]                  \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 2531)   761831      decoder[3][0]                    \n",
      "==================================================================================================\n",
      "Total params: 2,242,331\n",
      "Trainable params: 2,242,331\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(img_input):\n",
    "    if img_input.shape != (1, 512):\n",
    "        img_input = img_input.reshape(1, 512)\n",
    "    \n",
    "    assert(img_input.shape == (1, 512))\n",
    "    stop_condition = False\n",
    "    decoded_sentence = []\n",
    "    target_seq = np.array([token2idx['<bos>']]).reshape(1, 1)\n",
    "    states_value = encoder_model.predict(img_input)\n",
    "\n",
    "    while not stop_condition:\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        sampled_token_index = int(np.argmax(output_tokens[0, -1, :]))\n",
    "\n",
    "        sampled_char = idx2token[sampled_token_index]\n",
    "\n",
    "        decoded_sentence += [sampled_char]\n",
    "\n",
    "        if (sampled_char == '<eos>' or len(decoded_sentence) > 30):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.array([sampled_token_index]).reshape(1, 1)\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return ' '.join(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'two dogs dog dog <eos>'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_seq(train_encoder_output[1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
